{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8e74d0-0de2-4cb8-99b8-f942c4f264a5",
   "metadata": {},
   "source": [
    "# Procure to Pay data transformation lab\n",
    "In this example, we are going to transform a few data files provided by a client, into a multi-level process event-log that can be loaded and analyzed in IBM Process Mining.\n",
    "\n",
    "This data transformation process can be done using various techniques and tools, and it is really a strength that clients can apply their existing data practices and skills to feed IBM Process Mining. \n",
    "\n",
    "The exact same transformation process is also illustrated in another lab using IBM SPSS. We could be using IBM Datastage, Talend, and so forth.\n",
    "\n",
    "The goal of this example is to show how easy it is to do this work using Python language and Pandas library. Obviously, basic programming skills are required, but you will see that handling tables with Pandas (called dataframes) is quite simple, and that you can reuse parts of this example in your own project.\n",
    "\n",
    "## Installing and running Jupyterlab\n",
    "You need to have Python installed to run Jupyterlab locally. \n",
    "These steps should be sufficient to install Python and pandas on a Linux host:\n",
    "\n",
    "`yum install -y python36-devel.x86_64`\n",
    "\n",
    "`pip3 install jupyterlab`\n",
    "\n",
    "`pip3 install pandas`\n",
    "\n",
    "Then run:\n",
    "\n",
    "`jupyter lab --allow-root`\n",
    "\n",
    "Similar instructions exist for installing on Mac or Windows.\n",
    "\n",
    "## What is Pandas\n",
    "Pandas is an open source Python package that is most widely used for data science/data analysis and machine learning tasks. It is built on top of another package named Numpy, which provides support for multi-dimensional arrays. As one of the most popular data wrangling packages, Pandas works well with many other data science modules inside the Python ecosystem, and is typically included in every Python distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70602ed9-ea1a-4961-a5c2-f290eff72be7",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "In this example, the client provided 3 CSV files that we want to transform to create a multi-level process event log for Procure To Pay (similar to the P2P event log used in the Process Mining Tutorials).\n",
    "\n",
    "* requisition.csv contains the purchase requisition data set that needs to be transformed into an event log.\n",
    "\n",
    "* procurement.csv contains ready-to-use events\n",
    "\n",
    "* invoice.csv contains additional business data for invoices, that we want to add to the procurement events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdf1a9-3a01-41be-8a30-381c8bcfe106",
   "metadata": {},
   "source": [
    "## requisition.csv\n",
    "The requisition.csv file is not ready to be used by Process Mining. Indeed, it does not contain an activity column where activities are listed, nor does it contain a proper startDate column. Let's see what it looks like by loading it with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bd29e-6ea1-4fd3-80ef-f56a3a9f8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "requisitions = pd.read_csv('requisition.csv')\n",
    "requisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c195d-d64e-4ed7-a711-0455848421d6",
   "metadata": {},
   "source": [
    "The file includes 241133 requisitions, each with a Req_ID that indentifies each case.\n",
    "\n",
    "There are two column named respectively 'Create_Date' and 'Release_DateTime' that represent the 'requisition creation' activities and dates, and 'requisition release' activity and date. They are associated with users, roles, type, and source specific to each activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fca957-dafb-4625-ab8e-35549c582d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "requisitions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb114d-47b3-4b01-8628-61bc9bb8399f",
   "metadata": {},
   "source": [
    "We are going to create the events for `Requisition Created` activity, and for `Requisition Released' activity`. We just need to duplicate the original dataset such that we have one event for each activity (remember, there are 2 columns that represent the activity dates), to add an 'activity' column and to rename a few columns.\n",
    "\n",
    "### Requistion Created Events\n",
    "Let's first manage the '`Requisition Created` events.\n",
    "\n",
    "With Pandas, we copy the requisition dataframe, then we add an `activity` column. We fill this column with the string `Requisition Created`.\n",
    "\n",
    "The field `Create_Date` becomes the `datetime` column that we keep for the entire P2P event log, as a start date. `Create_User` and `Create_Role` become respectively `user` and `role`.\n",
    "\n",
    "We don't need to keep the fields and values related to the 'Release' activity in this dataset.\n",
    "\n",
    "With Pandas, we use `dataframe.rename()` and `dataframe.drop()` functions. \n",
    "\n",
    "Alternatively, we could have created a new column `datetime`, and we would have filled it with the `Create_Date` column like this; `create_requisitions['activity'] = create_requisitions['Create_Date']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c188e64-2338-4586-ae79-ff081f7fcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_requisitions = requisitions.copy()\n",
    "create_requisitions['activity'] = 'Requisition Created'\n",
    "create_requisitions.rename(columns={'Create_Date' :'datetime', 'Create_User' : 'user', 'Create_Role' : 'role'}, inplace=True)\n",
    "create_requisitions.drop(['Release_DateTime','Release_User','Release_Role','Release_Type','Release_Source'], axis=1, inplace=True)\n",
    "create_requisitions.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f8787-f646-46af-972b-0d17b9871f79",
   "metadata": {},
   "source": [
    "### Requisition Released Events\n",
    "We basically do the same thing with the `Requisition Released` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5592ed-f086-4939-b0e6-64c6b3cae9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_requisitions = requisitions.copy()\n",
    "release_requisitions['activity'] = 'Requisition Released'\n",
    "release_requisitions.rename(columns={'Release_DateTime' :'datetime', 'Release_User':'user', 'Release_Role':'role'}, inplace=True)\n",
    "release_requisitions.drop(['Create_Date','Create_User','Create_Role','Create_Type','Create_Source'], axis=1, inplace=True)\n",
    "release_requisitions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29375cad-0dec-43ea-bdde-2666b35de8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have rows with nothing in the 'Create_Date' field? No. You can try the instruction below\n",
    "# if we had, we could remove these rows with requisition[requisition['Create_Date'].notna()]\n",
    "requisitions['Create_Date'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfb822-17ca-46ee-b34c-d3a99cdd39c7",
   "metadata": {},
   "source": [
    "### Append the 2 requisition events\n",
    "With Pandas, we can easily append dataframes using `dataframe.append()`. The function does a union of the columns, and merging common columns, which is exactly what we need here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baef0ca-a68f-43bb-aa80-abb6178a7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "requisition_events = create_requisitions.copy()\n",
    "requisition_events = requisition_events.append(release_requisitions)\n",
    "requisition_events.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3414a8-3058-4651-bdf4-ce6bd66df5f6",
   "metadata": {},
   "source": [
    "### Cleansing and saving the requisition events\n",
    "In this dataset, each Req_ID has indeed the two activities. But it could happen that a Req_ID would not have the release information if this is a running case that has not yet reached this phase.\n",
    "\n",
    "In this case, we would need to remove all the rows with a null value in the `datetime`column. That is done with the following code : `requisition_events[requisition_events['datetime'].notna()]`\n",
    "\n",
    "`requisition_events['datetime'].notna()` returns False if the date is null. It acts as a filter with which we remove all the rows with a null date.\n",
    "\n",
    "Let's see if there are some null dates, the function below should return 0 if no such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de1aee-8ddd-4584-9d81-5d5e3b5722b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "requisition_events['datetime'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22892f62-b805-43b6-9155-2a7301d3868f",
   "metadata": {},
   "source": [
    "Finally we save the event log into a CSV file, and we keep the dataframe to append it later with the procurement dataset. The parameter Ã¬ndex=None' is needed to remove the first column (the index) that contains indexes for each row, which is not needed for process mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888e78-9099-4cd5-afd2-4ac9f6ac6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requisition_events[requisition_events['datetime'].notna()] # just in case there are null dates\n",
    "requisition_events.convert_dtypes() # Convert columns to best possible dtypes using dtypes supporting\n",
    "requisition_events.to_csv('requisition_events.csv', index=None)\n",
    "requisition_events # print the event log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60e813-a66c-4204-ae93-bf146422a8a3",
   "metadata": {},
   "source": [
    "## Procurement events\n",
    "The procurement events includes all the events from purchase order, good receipts, and invoices.\n",
    "\n",
    "We could leave it as is, but instead, we want to complement the events with the information that is stored in invoice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aef967-a5f4-499e-a8af-c0e770c03dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "procurements = pd.read_csv('procurement.csv',low_memory=False)\n",
    "procurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806929d7-fa0b-497c-bb71-73ac36a4ec2c",
   "metadata": {},
   "source": [
    "### Invoices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806af2a9-c6ee-4c4f-958a-07a2b4f5d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices = pd.read_csv('invoice.csv')\n",
    "invoices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd29e18-c8b4-4783-8371-770f198c22b6",
   "metadata": {},
   "source": [
    "## Merging invoice.csv information into procurement.csv\n",
    "We are going to add the invoice information to each event that relates to an invoice. To do that, we are going to use the key `Invoice_ID` to merge the invoice info to the invoice event, when the `Invoice_ID` columns match.\n",
    "\n",
    "This would be a typical joint in a database. With Pandas, we use the `dataframe.merge()` function where we set the key, and the how=\"left\" which means that we keep the procurement_events dataframe that we enrich with the invoice information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd792b-abef-4c12-9b95-bdd3179a4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "procurement_events = procurements.merge(invoices, on=\"Invoice_ID\", how=\"left\")\n",
    "procurement_events.convert_dtypes() # Convert columns to best possible dtypes using dtypes supporting\n",
    "procurement_events.to_csv('procurement_events.csv', index=None)\n",
    "procurement_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632772fa-6b4d-4b8e-aee6-f960a4e95c8f",
   "metadata": {},
   "source": [
    "## Final Step: Create the P2P event log\n",
    "Finally we append the requisition and the procurement event logs to create the final event log. Again, we can remove the events with a null `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c61bf3-4845-4d91-9b68-031f685dbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2P_events = requisition_events.copy()\n",
    "P2P_events = P2P_events.append(procurement_events)\n",
    "P2P_events = P2P_events[P2P_events[\"datetime\"].notna()] # removing rows with no datetime if any\n",
    "P2P_events = P2P_events.convert_dtypes() # applying the best known types\n",
    "P2P_events.to_csv('P2P_events.csv', index=None)\n",
    "P2P_events.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f578f4b-5e53-4a40-a538-bfff78657fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2P_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddcfab-47de-4fa5-84ee-c3812b3840dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
